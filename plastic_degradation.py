# -*- coding: utf-8 -*-
"""Plastic Degradation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SjAyd3FCELFybYA8Y3K4al9EBDBepaa9
"""

"""
#Project

###PlasticDB Microorganisms

#####Scikit-Learn Approach
"""

!pip install pandas scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

df = degraders_df[['Microorganism', "Plastic",'Sequence', 'Degradation extrapolated from enzyme']].dropna()
df

# Load the dataset
degraders_df = pd.read_csv('/content/degraders_list.tsv', sep='\t')

# Select relevant columns and drop missing values
df = degraders_df[['Sequence', 'Degradation extrapolated from enzyme']].dropna()

# Convert the target variable to binary (Yes/No -> 1/0)
df['Degradation'] = df['Degradation extrapolated from enzyme'].apply(lambda x: 1 if x == 'Yes' else 0)

# Use CountVectorizer to transform sequences into features
vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 4))
X = vectorizer.fit_transform(df['Sequence'])

# Encode the target variable
y = df['Degradation'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the RandomForest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Display the results
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

"""#####Different Approach"""

!pip install tensorflow pandas numpy scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout

# Load the dataset
degraders_df = pd.read_csv('degraders_list.tsv', sep='\t')

# Select relevant columns and drop missing values
df = degraders_df[['Sequence', 'Degradation extrapolated from enzyme']].dropna()

# Convert the target variable to binary (Yes/No -> 1/0)
df['Degradation'] = df['Degradation extrapolated from enzyme'].apply(lambda x: 1 if x == 'Yes' else 0)

# Tokenize the sequences
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(df['Sequence'])
sequences = tokenizer.texts_to_sequences(df['Sequence'])

# Pad sequences to have the same length
max_len = max(len(seq) for seq in sequences)
X = pad_sequences(sequences, maxlen=max_len)

# Encode the target variable
y = df['Degradation'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model architecture
model = Sequential()

# Embedding layer
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 128
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))

# Convolutional layer
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))

# Bidirectional LSTM layer
model.add(Bidirectional(LSTM(64)))

# Fully connected layers
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))

# Predict on the test set
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

# Additional metrics
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Classification Report:\n{report}")
print(f"Confusion Matrix:\n{conf_matrix}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""#####Scikit-learn K-Means Clustering"""

pip install biopython tensorflow deepchem

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('degraders_list.tsv', sep='\t')

# Convert categorical columns to numerical features
df['Plastic_Type'] = df['Plastic'].astype('category').cat.codes
df['Degrades_Plastic'] = df['Degradation extrapolated from enzyme'].astype('category').cat.codes

# Create numerical features (example: use Tax ID and encoded Plastic Type)
features = df[['Tax ID', 'Plastic_Type']]

# Optional: Add additional numerical features if available
# e.g., df['Feature1'] = ...

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

# Clustering: Apply KMeans clustering
kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust n_clusters as needed
clusters = kmeans.fit_predict(X_scaled)
df['Cluster'] = clusters

# Split the data into training and testing sets
X = X_scaled
y_degrades = df['Degrades_Plastic']
y_plastic_type = df['Plastic_Type']

X_train, X_test, y_train_degrades, y_test_degrades, y_train_plastic_type, y_test_plastic_type = train_test_split(
    X, y_degrades, y_plastic_type, test_size=0.1, random_state=42
)

# Classification model: Random Forest Classifier
rf_clf_degrades = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf_degrades.fit(X_train, y_train_degrades)
y_pred_degrades = rf_clf_degrades.predict(X_test)

rf_clf_plastic_type = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf_plastic_type.fit(X_train, y_train_plastic_type)
y_pred_plastic_type = rf_clf_plastic_type.predict(X_test)

# Evaluate the classification models
accuracy_degrades = accuracy_score(y_test_degrades, y_pred_degrades)
accuracy_plastic_type = accuracy_score(y_test_plastic_type, y_pred_plastic_type)

print(f"Degrades Plastic Classification Accuracy: {accuracy_degrades}")
print(f"Plastic Type Classification Accuracy: {accuracy_plastic_type}")

print("Degrades Plastic Classification Report:")
print(classification_report(y_test_degrades, y_pred_degrades))

print("Plastic Type Classification Report:")
print(classification_report(y_test_plastic_type, y_pred_plastic_type))

"""#####XGBoost"""

!pip install xgboost

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
import xgboost as xgb

# 1. Load the dataset
df = pd.read_csv('degraders_list.tsv', sep='\t')

# 2. Data Preprocessing

# Handle missing values with imputer (use strategy depending on the column type)
imputer = SimpleImputer(strategy='most_frequent')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Convert 'Tax ID' to numeric, forcing errors to NaN
df_imputed['Tax ID'] = pd.to_numeric(df_imputed['Tax ID'], errors='coerce')

# Fill any NaNs in 'Tax ID' after conversion with the median
df_imputed['Tax ID'].fillna(df_imputed['Tax ID'].median(), inplace=True)

# Encoding categorical variables
label_encoders = {}
for column in ['Plastic', 'Degradation extrapolated from enzyme']:
    le = LabelEncoder()
    df_imputed[column] = le.fit_transform(df_imputed[column])
    label_encoders[column] = le

# Check encoded values
print("Encoded 'Plastic' classes:", label_encoders['Plastic'].classes_)
print("Encoded 'Degradation extrapolated from enzyme' classes:", label_encoders['Degradation extrapolated from enzyme'].classes_)

# 3. Feature Engineering: Adding meaningful features
df_imputed['Tax_ID_log'] = np.log1p(df_imputed['Tax ID'])  # Log-transforming numeric data safely

# Check if 'Sequence Length' exists before trying to create derived features
if 'Sequence Length' in df_imputed.columns:
    df_imputed['Sequence_Length_Square'] = df_imputed['Sequence Length'].astype(float) ** 2
    features = df_imputed[['Tax_ID_log', 'Sequence_Length_Square', 'Plastic']]
else:
    features = df_imputed[['Tax_ID_log', 'Plastic']]

# Select the target variable
target = df_imputed['Degradation extrapolated from enzyme']

# 4. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.1, random_state=42)

# 5. Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6. Model Training with XGBoost
xgb_clf = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42)
xgb_clf.fit(X_train_scaled, y_train)

# 7. Predictions and Evaluation
y_pred = xgb_clf.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Classification Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
'''
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoders['Degradation extrapolated from enzyme'].classes_, yticklabels=label_encoders['Degradation extrapolated from enzyme'].classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# 8. Feature Importance
xgb.plot_importance(xgb_clf, max_num_features=10)
plt.show()'''

"""###PlasticDB & PMDB Enzyme Sequences"""

!pip install pandas biopython

extended_plastic_abbreviation = {
    "Polyvinylalcohol dehydrogenase": "PVA",    # Polyvinyl Alcohol
    "PLA depolymerase": "PLA",                  # Polylactic Acid
    "Poly(3-hydroxybutyrate) depolymerase": "PHB",  # Poly(3-hydroxybutyrate)
    "Poly(3-hydroxyalkanoate) depolymerase": "PHA", # Poly(3-hydroxyalkanoate)
    "Polyurethanase": "PUR",                    # Polyurethane
    "Poly(ethylene terephthalate) hydrolase": "PET", # Polyethylene Terephthalate
    "Phthalate dioxygenase": "PHT",             # Phthalates
    "PETase": "PET",                            # Polyethylene Terephthalate
    "Polypropylene hydrolase": "PP",            # Polypropylene
    "Polystyrene dioxygenase": "PS",            # Polystyrene
    "Polycarbonate hydrolase": "PC",            # Polycarbonate
    "Polyvinyl chloride hydrolase": "PVC"       # Polyvinyl Chloride
}

# Function to determine the plastic type based on enzyme name, with extended matching.
def determine_detailed_plastic_type(enzyme_name):
    for key, abbreviation in extended_plastic_abbreviation.items():
        if key.lower() in enzyme_name.lower():
            return abbreviation
    # Additional detailed refinement based on more specific keywords
    if "dioxygenase" in enzyme_name.lower():
        return "PHT"  # Assuming phthalates if the term is generic
    elif "depolymerase" in enzyme_name.lower():
        return "PHA"  # General assumption if it's a depolymerase
    elif "hydrolase" in enzyme_name.lower():
        if "polyethylene" in enzyme_name.lower():
            return "PET"
        elif "polypropylene" in enzyme_name.lower():
            return "PP"
        elif "polycarbonate" in enzyme_name.lower():
            return "PC"
        elif "polyvinyl chloride" in enzyme_name.lower() or "pvc" in enzyme_name.lower():
            return "PVC"
    return "Other"  # Return "Other" if no detailed match is found

import pandas as pd
from Bio import SeqIO

# Initialize lists to store data
identifiers = []
enzymes = []
organisms = []
tax_ids = []
genes = []
plastics = []
sequences = []
sequence_lengths = []

# Parsing function for hmmermaterial.fasta
def parse_hmmermaterial_header(header):
    parts = header.split()

    # Extract Identifier
    identifier = parts.pop(0)

    # Extract Enzyme
    enzyme = ""
    while "OS=" not in parts[0]:
        enzyme += parts.pop(0)
        enzyme += " "
    enzyme = enzyme.strip()

    # Extract Organism
    organism = parts.pop(0)[3:]
    while "OX=" not in parts[0]:
        organism += " " + parts.pop(0)
    organism = organism.strip()

    # Extract Tax_id
    tax_id = parts.pop(0)[3:]

    # Extract Gene
    gene = parts.pop(0)[3:]

    return identifier, enzyme, organism, tax_id, gene, determine_detailed_plastic_type(enzyme)

# Parsing function for PlasticDB.fasta
def parse_plasticdb_header(header):
    parts = header.split('||')

    # Extract Identifier
    identifier = parts[0]

    # Extract Enzyme
    enzyme = " ".join(parts[1].split("_"))

    # Extract Organism
    organism = " ".join(parts[2].split("_"))

    # Extract Plastic
    plastic = " ".join(parts[3].split("_"))

    return identifier, enzyme, organism, "NaN", "NaN", plastic

# Load and parse sequences from hmmermaterial.fasta
for record in SeqIO.parse('/content/hmmermaterial.fasta', "fasta"):
    header = record.description
    identifier, enzyme, organism, tax_id, gene, plastic = parse_hmmermaterial_header(header)

    identifiers.append(identifier)
    enzymes.append(enzyme)
    organisms.append(organism)
    tax_ids.append(tax_id)
    genes.append(gene)
    plastics.append(plastic)
    sequence = str(record.seq)
    sequences.append(sequence)
    sequence_lengths.append(len(sequence))

# Load and parse sequences from PlasticDB.fasta
for record in SeqIO.parse('/content/PlasticDB.fasta', "fasta"):
    header = record.description
    identifier, enzyme, organism, tax_id, gene, plastic = parse_plasticdb_header(header)

    identifiers.append(identifier)
    enzymes.append(enzyme)
    organisms.append(organism)
    tax_ids.append(tax_id)
    genes.append(gene)
    plastics.append(plastic)
    sequence = str(record.seq)
    sequences.append(sequence)
    sequence_lengths.append(len(sequence))

# Create DataFrame with extracted information
df = pd.DataFrame({
    'Identifier': identifiers,
    'Enzyme': enzymes,
    'Organism': organisms,
    'Tax_id': tax_ids,
    'Gene': genes,
    'Plastic': plastics,
    'Sequence': sequences
})

# Save the DataFrame as a CSV file
csv_file_path = '/content/PlasticDB & PMDB Enzyme Information.csv'
df.to_csv(csv_file_path, index=False)

"""###Plastic Statistics"""

!pip install biopython

import pandas as pd
import re

# Load data from files
csv_file_path = '/content/PlasticDB & PMDB Enzyme Information.csv'
tsv_file_path = '/content/degraders_list.tsv'

# Read the data into dataframes
df_csv = pd.read_csv(csv_file_path)
df_tsv = pd.read_csv(tsv_file_path, sep='\t')

# Filter the data
successful_degraders_tsv = df_tsv[df_tsv['Degradation extrapolated from enzyme'] == 'Yes']
successful_degraders_csv = df_csv[df_csv['Plastic'] != "Other"].reset_index(drop=True)

# Function to split plastics and expand rows
def split_plastics(df, plastic_col):
    df = df.copy()
    df[plastic_col] = df[plastic_col].str.split()
    df = df.explode(plastic_col)
    return df

# Apply the splitting function to both dataframes
tsv = split_plastics(successful_degraders_tsv, 'Plastic')
csv = split_plastics(successful_degraders_csv, 'Plastic')

# Create market share dictionary
market_share_dict = {'PE': 25.0, 'PET': 12.0, 'PU': 7.0, 'PS': 7.0, 'NYLON': 4.5, 'PLA': 0.2, 'PVA': 0.1, 'PHA': 0.1, 'PBAT': 0.1, 'PBS': 0.1, 'PEF': 0.05, 'PPL': 19.0, 'PBS-BLEND': 0.05, 'PBSA-BLEND': 0.05, 'P(3HB-CO-3MP)': 0.01, 'ECOVIO-FT': 0.01, 'P3HV': 0.01, 'PHBH': 0.01, 'PHBVH': 0.01, 'PHN': 0.01, 'PMCL': 0.01, 'PBSET': 0.01, 'PHPV': 0.01, 'PHB': 0.1, 'LDPE': 17.0, 'PHO': 0.01, 'O-PVA': 0.1, 'NR': 3.0, 'PEG': 0.05, 'P3HP': 0.05, 'P4HB': 0.05, 'PEA': 0.1, 'PHBV': 0.1, 'PES': 1.0, 'PBSA': 0.1, 'PCL': 0.3, 'PUR': 7.0, 'PHT': 0.01, 'O-PE': 0.05}

plastics = ['PE', 'PET', 'PU', 'PS', 'NYLON', 'PLA', 'PVA', 'PHA', 'PBAT', 'PBS', 'PEF', 'PPL', 'PBS-BLEND', 'PBSA-BLEND', 'P(3HB-CO-3MP)', 'ECOVIO-FT', 'P3HV', 'PHBH', 'PHBVH', 'PHN', 'PMCL', 'PBSET', 'PHPV', 'PHB', 'LDPE', 'PHO', 'O-PVA', 'NR', 'PEG', 'P3HP', 'P4HB', 'PEA', 'PHBV', 'PES', 'PBSA', 'PCL', 'PUR', 'PHT', 'O-PE']
df = pd.DataFrame(columns=['Plastic', 'Species Count', 'Sequence Count', 'Market Share'])

for plastic in plastics:
  organisms = []
  sequences = []
  for i in csv.itertuples(index=True):
    if i.Plastic == plastic:
      organisms.append(i.Organism)
      sequences.append(i.Sequence)
  for i in tsv.itertuples(index=True):
    if i.Plastic == plastic:
      organisms.append(i.Microorganism)
      sequences.append(i.Sequence)
  df.loc[len(df)] = [plastic, len(list(set(organisms))), len(list(set(sequences))), market_share_dict[plastic]]

# Sort the final dataframe by Market Share in descending order
df = df.sort_values(by='Market Share', ascending=False)
df = df.reset_index(drop=True)

print(df)

"""#Rahul Work

##PlasmoDB Sequence Extraction
"""

!pip install biopython

import pandas as pd
from Bio import Entrez, SeqIO
import time

Entrez.email = "sah0501@dcds.edu"

# Load the data from the CSV file
file_path = '/content/testing_WithGraph.csv'
data = pd.read_csv(file_path)

# Extract unique PlasmoDB Source IDs
plasmo_db_ids = data['Gene ID'].unique()

# Function to fetch genetic sequence using PlasmoDB ID
def fetch_sequence(plasmo_id):
    try:
        # Search for the ID in the nucleotide database
        search_handle = Entrez.esearch(db="nucleotide", term=plasmo_id, retmode="xml")
        search_results = Entrez.read(search_handle)
        search_handle.close()

        if search_results['Count'] == '0':
            print(f"No records found for ID {plasmo_id}")
            return None

        id_list = search_results['IdList']
        if not id_list:
            print(f"No matching IDs found for {plasmo_id}")
            return None

        # Fetch the sequence
        fetch_handle = Entrez.efetch(db="nucleotide", id=id_list[0], rettype="fasta", retmode="text")
        record = SeqIO.read(fetch_handle, "fasta")
        fetch_handle.close()

        return str(record.seq)

    except Exception as e:
        print(f"Error fetching data for ID {plasmo_id}: {e}")
        return None

# Initialize lists to store the results
sequences = []

# Fetch data for each PlasmoDB ID
for plasmo_id in plasmo_db_ids:
    sequence = fetch_sequence(plasmo_id)
    if sequence:
        sequences.append(sequence)
    else:
        sequences.append(None)

# Create a DataFrame from the results
results_df = pd.DataFrame({
    'PlasmoDB ID': plasmo_db_ids,
    'Genetic Sequence': sequences
})

# Save the DataFrame to a CSV file (optional)
results_df.to_csv('/content/plasmo_db_sequences.csv', index=False)

# Print the DataFrame
print(results_df)

"""##Code

###config.py
"""

!pip install torch
!pip install transformers

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB

from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
import bertSklearnWrapper
from datetime import date

today = date.today()

CLASSIFIER= LogisticRegression(
    C=5e1,
    solver='lbfgs',
    multi_class='multinomial',
    class_weight='balanced' ,
    random_state=42, n_jobs=1
)

VECTORIZER= TfidfVectorizer(
    stop_words='english',
    ngram_range=(1, 2),  #uni-grams and bi-grams
    lowercase=True,
    max_features=150000
)

vectorizerBERT = bertSklearnWrapper.BertTransformer(
    DistilBertTokenizer.from_pretrained("distilbert-base-uncased"),
    DistilBertModel.from_pretrained("distilbert-base-uncased"),
    embedding_func=lambda x: x[0][:, 0, :].squeeze()
)

SPLIT= 0.2

TRAINING_DATA= "/content/training_WithGraph.csv"
TESTING_DATA= "/content/testing_WithGraph.csv"
INPUT_COLUMNS= ["Product Description","Gene Name or Symbol" ,"GraphToColumn_readable"]
TARGET_COLUMN= "Pathogenicity"

MODEL_SAVE_PATH= f'model/model_{today.strftime("%B %d, %Y")}.pkl'
SAVED_MODEL= 'model/model_August 24, 2021.pkl'

PREDICTIONS_SAVE_PATH= f'runs/predictions_{today.strftime("%B %d, %Y")}.csv'
REPORTS_SAVE_PATH= f'runs/report_{today.strftime("%B %d, %Y")}.pdf'

"""###prepare_data.py"""

import pandas as pd
import numpy as np
import config

class PrepareData:
    def __init__(self, data):
        '''
        data is pandas df with required input columns + target
        '''
        self.data= data


    def prep(self, data):
        #replace NaN
        for col in data.columns:
            data[col] =data[col].replace(np.nan, "@")

        #join the input text columns
        data['text']= data[config.INPUT_COLUMNS].agg(' '.join, axis=1)

        #remove empty input col values
        data['text']= data['text'].replace(to_replace=r'@\.', value="", regex=True)
        data['text']= data['text'].replace(to_replace=r'\s+@', value="", regex=True)
        data['text']= data['text'].replace(to_replace=r'\s\s', value="", regex=True)

        return data

    def getData(self):
        return self.prep(self.data)

"""###quickGo.py"""

import requests, sys
import re
import numpy as np
import ast
from tqdm import tqdm
import pandas as pd

cache= {}
def getResponse(GO_id):
    if not GO_id or not isinstance(GO_id, str):
        return ""
    if GO_id in cache:
        return cache[GO_id]
    try:
        GO_id= GO_id.replace(":", "%3A") #URL Encode
        requestURL= f'https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/graph?startIds={GO_id}'
        response = requests.get(requestURL, headers={ "Accept" : "application/json"})
        GO_id= GO_id.replace("%3A", ":")
        res= response.json()['results']
        cache[GO_id]= res
        return res
    except requests.exceptions.HTTPError as errh:
        print(errh)
    except requests.exceptions.ConnectionError as errc:
        print(errc)
    except requests.exceptions.Timeout as errt:
        print(errt)
    except requests.exceptions.RequestException as err:
        print(err)

def getTree(df):
    curated_GO_Functions= re.compile("Curated GO Function IDs-\d")
    curated_GO_Components= re.compile("Curated GO Component IDs-\d")
    curated_GO_Process= re.compile("Curated GO Process IDs-\d")

    all_curated= list(filter(curated_GO_Components.match, df.columns))
    all_curated.extend(list(filter(curated_GO_Functions.match, df.columns)))
    all_curated.extend(list(filter(curated_GO_Process.match, df.columns)))

    for _, column in enumerate(tqdm(all_curated)):
        new_column_name= f'{column}_graph'
        df[new_column_name]= df[column].apply(lambda x: getResponse(x))

    df.to_csv("/content/testing_WithGraph.csv")

def GraphToColumn(df):
    new_column_name= "GraphToColumn"
    matchRegex= re.compile('.*_graph')
    lists_= list(
        filter(matchRegex.match, df.columns)
    )
    df[new_column_name]= pd.Series([]*len(df), dtype="float64")
    df[new_column_name]= df[new_column_name].apply(
        lambda x: []
    )
    #literal eval
    for _, col in enumerate(lists_):
        df[col]= df[col].replace(np.nan, "[]")
        df[col]= df[col].apply(lambda x: ast.literal_eval(x))

    for ind, row in df.iterrows():
        outer_res= []
        for _, targetCol in enumerate(lists_):
            try:
                res= list(
                    map(
                        lambda x: x['label'], df[targetCol].iloc[ind][0]['vertices']
                    )
                )
                outer_res.extend(res)

            except IndexError:
                continue
        df[new_column_name].iloc[ind]= list(set(outer_res))


    df[f'{new_column_name}_readable']= df[new_column_name].apply(lambda x: " ".join(x))
    print(df.iloc[1])
    df.to_csv("/content/testing_WithGraph.csv")



if __name__ == "__main__":
    df= pd.read_csv("/content/testing_WithGraph.csv")
    GraphToColumn(df)

"""###bertSklearnWrapper.py"""

from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer
from typing import Callable, List, Optional, Tuple
from sklearn.base import TransformerMixin, BaseEstimator
import torch
import pandas as pd

class BertTransformer(BaseEstimator, TransformerMixin):
    '''
    Code Credit - @nbertagnolli [The Sklearn Wrapper code, Line 5 through 56]
    LINK - https://gist.github.com/nbertagnolli/3dd87f8702570749dc7c111ae8997189
    '''
    def __init__(
            self,
            bert_tokenizer,
            bert_model,
            max_length: int = 60,
            embedding_func: Optional[Callable[[torch.tensor], torch.tensor]] = None,
    ):
        self.tokenizer = bert_tokenizer
        self.model = bert_model
        self.model.eval()
        self.max_length = max_length
        self.embedding_func = embedding_func

        if self.embedding_func is None:
            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()

    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:
        # Tokenize the text with the provided tokenizer
        tokenized_text = self.tokenizer.encode_plus(text,
                                                    add_special_tokens=True,
                                                    max_length=self.max_length
                                                    )["input_ids"]

        # Create an attention mask telling BERT to use all words
        attention_mask = [1] * len(tokenized_text)

        # bert takes in a batch so we need to unsqueeze the rows
        return (
            torch.tensor(tokenized_text).unsqueeze(0),
            torch.tensor(attention_mask).unsqueeze(0),
        )

    def _tokenize_and_predict(self, text: str) -> torch.tensor:
        tokenized, attention_mask = self._tokenize(text)

        embeddings = self.model(tokenized, attention_mask)
        return self.embedding_func(embeddings)

    def transform(self, text: List[str]):
        if isinstance(text, pd.Series):
            text = text.tolist()

        with torch.no_grad():
            return torch.stack([self._tokenize_and_predict(string) for string in text])

    def fit(self, X, y=None):
        return self

"""###clusterAlgo.py"""

import numpy as np
import pandas as pd
import prepare_data
import config
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score

class clust():
    def _load_data(self):
        data = pd.read_csv(
            "/content/training_WithGraph.csv",
            on_bad_lines='skip'
        )

        dataObj= prepare_data.PrepareData(
            data
        )

        data= dataObj.getData()
        data.dropna(subset=['text'], inplace=True)

        #Don't train nor predict for psuedogenes
        data= data[~data["text"].str.contains('pseudogene')]

        #prepare data for model training and testing
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            data["text"], data[config.TARGET_COLUMN], test_size = config.SPLIT, random_state = 42
        )

        vectorizer = TfidfVectorizer()
        vectorizer = vectorizer.fit(self.X_train)
        self.X_train= vectorizer.transform(self.X_train)
        self.X_test= vectorizer.transform(self.X_test)


    def __init__(self):
        self._load_data()


    def classify(self, model=LogisticRegression(random_state=42)):
        model.fit(self.X_train, self.y_train)
        y_pred = model.predict(self.X_test)
        print('Accuracy: {}'.format(accuracy_score(self.y_test, y_pred)))
        return self

    def Kmeans(self, output='add'):
        n_clusters = len(np.unique(self.y_train))
        self.clf = KMeans(n_clusters = n_clusters, random_state=42)
        self.clf.fit(self.X_train)
        y_labels_train = self.clf.labels_
        y_labels_test = self.clf.predict(self.X_test)

        if output == 'add':
            y_labels_train = y_labels_train[:, np.newaxis]
            y_labels_test = y_labels_test[:, np.newaxis]
            self.X_train= np.hstack((self.X_train.toarray(), y_labels_train))
            self.X_test = np.hstack((self.X_test.toarray(), y_labels_test))
        elif output == 'only':
            print('Accuracy: {}'.format(accuracy_score(self.y_test, y_labels_test)))
        elif output == "replace":
            self.X_train = y_labels_train[:, np.newaxis]
            self.X_test = y_labels_test[:, np.newaxis]
        else:
            raise ValueError('output should be either add or replace')
        return self


print('Running for Kmeans added with TFIDF')
clust().Kmeans("add").classify()

print('\nRunning for only TFIDF')
clust().classify()

print('\nReplace TFIDF with Kmeans cluster heads')
clust().Kmeans("replace").classify()

"""###inference.py"""

!pip install bottle

import pandas as pd
import bottle
import prepare_data
import argparse
import numpy as np
import config
from pickle import load

def predict(model, text):
    prediction, confidence= model.predict([text])[0], max(model.predict_proba([text])[0])
    return prediction, confidence



if __name__ == "__main__":
    model = load(
        open("/content/model_August 24, 2021.pkl", 'rb')
    )

    data= pd.read_csv("/content/testing_WithGraph.csv")

    dataObj= prepare_data.PrepareData(
        data
    )


    data= dataObj.getData()
    data= data[~data["text"].str.contains('pseudogene')]

    #run for predictions
    prediction, confidence= zip(
        *(
            list(
                data['text'].apply(
                    lambda x: predict(model, x)
                )
            )
        )
    )

    data["prediction"]= prediction
    data["confidence"]= confidence


    #data.to_csv(config.PREDICTIONS_SAVE_PATH)

"""###inference_server.py"""

import bottle
from inference import predict
import argparse
import config
import pandas as pd
from pickle import load

@bottle.post("/predict")
def predict_server():
    text= bottle.request.json["text"]
    # curated_function= bottle.request.json["function"]
    # curated_process= bottle.request.json['process']
    # curated_component= bottle.request.json['component']
    # text= description+". "+curated_function+". "+curated_process+". "+curated_component
    prediction, confidence= predict(model, text)
    return {"Prediction": str(prediction), "Confidence": confidence}

if __name__=="__main__":
    model= load(
        open("/content/model_August 24, 2021.pkl", 'rb')
    )
    bottle.run(host="localhost", port=80)

"""###main.py"""

import config
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from report import report
from pickle import dump
import prepare_data
from sklearn.pipeline import Pipeline


if __name__ == "__main__":
    data= pd.read_csv(
        config.TRAINING_DATA,
        error_bad_lines= True
    )

    dataObj= prepare_data.PrepareData(
        data
    )

    data= dataObj.getData()
    data.dropna(subset=['text'], inplace=True)

    #Don't train nor predict for psuedogenes
    data= data[~data["text"].str.contains('pseudogene')]

    #prepare data for model training and testing
    X_train, X_test, y_train, y_test = train_test_split(
        data["text"], data[config.TARGET_COLUMN], test_size = config.SPLIT, random_state = 42
    )

    #model pipeline
    pipe = Pipeline(
        [
            ("vectorizer", config.vectorizerBERT),
            ("classifier", config.CLASSIFIER),
        ]
    )

    pipe.fit(X_train.tolist(), y_train)

    #save trained model
    dump(
        pipe, open(config.MODEL_SAVE_PATH, 'wb')
    )

    #get metrics
    reportObj= report(
        pipe,
        X_test,
        y_test
    )
    reportObj.compute()
    reportObj.draw()

"""###report.py"""

!pip uninstall -y scikit-plot
!pip install git+https://github.com/reiinakano/scikit-plot

!pip install scipy==1.4.1
import scipy
print(scipy.__version__)

import config
import scikitplot as skplt
from sklearn.metrics import classification_report
from reportlab.pdfgen.canvas import Canvas
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.units import inch
from reportlab.graphics import renderPDF
from reportlab.lib.utils import ImageReader
from reportlab.platypus import Paragraph, Frame, Table, Spacer, TableStyle
from svglib.svglib import svg2rlg
import pandas as pd
from io import StringIO, BytesIO

class report:
    def __init__(self, model, X_test, y_test):
        self.model= model
        self.X= X_test
        self.y= y_test
        self.Frame= Frame(
            inch,
            inch,
            6 * inch,
            9 * inch
        )

        self.canvas = Canvas(config.REPORTS_SAVE_PATH)

    def draw(self):
        story = [
            Paragraph("Report", getSampleStyleSheet()['Heading1']),
            Spacer(1, 20),
            self.component1

        ]

        self.canvas.drawImage(self.component2,  10, 10)
        self.Frame.addFromList(story, self.canvas)
        self.canvas.save()


    def compute(self):
        y_pred= self.model.predict(self.X)

        #for text
        report_= classification_report(
            self.y,
            y_pred,
            output_dict=True
        )

        df = pd.DataFrame(report_).transpose()
        df.reset_index(inplace=True)
        df = df.rename(columns = {'index':'Classes'})

        data = [df.columns.to_list()] + df.values.tolist()
        self.component1 = Table(data)
        self.component1.setStyle(
            TableStyle([
                ('INNERGRID', (0, 0), (-1, -1), 0.25, colors.black),
                ('BOX', (0, 0), (-1, -1), 0.25, colors.black)
            ])
        )

        #for images
        skpltDiagram= skplt.metrics.plot_confusion_matrix(
            self.y, y_pred,
            text_fontsize= 10,
            title_fontsize= 10,
            figsize=(4,4)
        )
        imgdata = BytesIO()
        skpltDiagram.figure.savefig(imgdata, format='png')
        imgdata.seek(0)  # rewind the data
        self.component2 = ImageReader(imgdata)
